{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Webscrapping with Beautiful Soup.\n",
    "\n",
    "This example shows how to download any ZIP file listed on a given webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "#import urllib\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current Working directory is C:\\source\\HACK\\chart-digitizer\n"
     ]
    }
   ],
   "source": [
    "# Create destination\n",
    "cwd = os.getcwd()\n",
    "datadir = cwd +\"/data\"\n",
    "print(\"The current Working directory is \" + cwd)\n",
    "if not os.path.exists(datadir):\n",
    "    os.mkdir(datadir) #, 0777);\n",
    "#print \"Created new directory \" + newdir\n",
    "#newfile = open('zipfiles.txt','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_find_zips(url):\n",
    "    \"\"\" scrape the given url for any zip files in href tags \"\"\"\n",
    "    response = requests.get(url)\n",
    "    #print(response.status_code)\n",
    "    #print(response.text)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    all_tags = soup.find_all('a', href=True)\n",
    "    tag_dict = {}\n",
    "    for tag in all_tags:\n",
    "        if '.tar.gz' in tag.get_text() or '.zip' in tag.get_text() or 'py2.py3-none-any.whl' in tag.get_text():\n",
    "            tag_dict[tag.get_text()] = tag['href']\n",
    "    return tag_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Reports/ARCHIVE/Daily_Reports/PUBLIC_DAILY_20170301.zip\n",
      "PUBLIC_DAILY_20170301.zip\n"
     ]
    }
   ],
   "source": [
    "base =  \"http://www.nemweb.com.au\"\n",
    "url = \"http://www.nemweb.com.au/Reports/Current/Daily_Reports/\" #Did previously\n",
    "url = \"http://www.nemweb.com.au/Reports/ARCHIVE/Daily_Reports/\"\n",
    "\n",
    "tag_dict = scrape_find_zips(url)\n",
    "\n",
    "print(next (iter (tag_dict.values())))\n",
    "print(next (iter (tag_dict.keys())))\n",
    "\n",
    "dest=os.getcwd()+'/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_downloader(url, urls, dest='./data/'):\n",
    "    \"\"\" download all files listed in urls {} dict \n",
    "    Input:\n",
    "        url = base url\n",
    "        urls= target paths to append to base url\n",
    "        dest= local path to store files\n",
    "    \"\"\"\n",
    "    for k_name, v_path in urls.items():\n",
    "        src = url+v_path\n",
    "        tgt = dest+k_name\n",
    "    \n",
    "        r = requests.get(src)\n",
    "        with open(tgt, 'wb') as code:\n",
    "            code.write(r.content)\n",
    "        print(src)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.nemweb.com.au/Reports/ARCHIVE/Daily_Reports/PUBLIC_DAILY_20170301.zip\n",
      "http://www.nemweb.com.au/Reports/ARCHIVE/Daily_Reports/PUBLIC_DAILY_20170401.zip\n",
      "http://www.nemweb.com.au/Reports/ARCHIVE/Daily_Reports/PUBLIC_DAILY_20170501.zip\n",
      "http://www.nemweb.com.au/Reports/ARCHIVE/Daily_Reports/PUBLIC_DAILY_20170601.zip\n",
      "http://www.nemweb.com.au/Reports/ARCHIVE/Daily_Reports/PUBLIC_DAILY_20170701.zip\n",
      "http://www.nemweb.com.au/Reports/ARCHIVE/Daily_Reports/PUBLIC_DAILY_20170801.zip\n",
      "http://www.nemweb.com.au/Reports/ARCHIVE/Daily_Reports/PUBLIC_DAILY_20170901.zip\n",
      "http://www.nemweb.com.au/Reports/ARCHIVE/Daily_Reports/PUBLIC_DAILY_20171001.zip\n",
      "http://www.nemweb.com.au/Reports/ARCHIVE/Daily_Reports/PUBLIC_DAILY_20171101.zip\n",
      "http://www.nemweb.com.au/Reports/ARCHIVE/Daily_Reports/PUBLIC_DAILY_20171201.zip\n",
      "http://www.nemweb.com.au/Reports/ARCHIVE/Daily_Reports/PUBLIC_DAILY_20180101.zip\n",
      "http://www.nemweb.com.au/Reports/ARCHIVE/Daily_Reports/PUBLIC_DAILY_20180201.zip\n",
      "http://www.nemweb.com.au/Reports/ARCHIVE/Daily_Reports/PUBLIC_DAILY_20180301.zip\n",
      "http://www.nemweb.com.au/Reports/ARCHIVE/Daily_Reports/PUBLIC_DAILY_20180401.zip\n"
     ]
    }
   ],
   "source": [
    "scrape_downloader(base, tag_dict, dest='./data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Setup\n",
    "Setup folder to  unzip  into\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remember that datadir = cwd +\"/data\"\n",
    "unzip_dir=os.getcwd()+'/data/unzip/'\n",
    "if not os.path.exists(unzip_dir):\n",
    "    os.mkdir(unzip_dir)\n",
    "    print('Created',unzip_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " \n",
    "f_zip = [datadir + '/' + x for x in os.listdir('./data/')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(f_zip[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "for f in f_zip[0:3]:  #remove [0] to extend loop\n",
    "    print(f)\n",
    "    if f.endswith(\".zip\"):\n",
    "        #Unzip\n",
    "        zip_ref = zipfile.ZipFile(f, 'r')\n",
    "        zip_ref.extractall(unzip_dir)\n",
    "        zip_ref.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(root)\n",
    "print(dirs)\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "f_unzip = os.listdir('./data/unzip/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(unzip_dir+f_unzip[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(f_unzip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process CSV file looking for specific header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "#----------------------------------------------------------------------\n",
    "def csv_reader(file_obj, segment=[\"I\",\"TUNIT\",\"\",\"1\"]):\n",
    "    \"\"\"\n",
    "    Read a csv file\n",
    "    scan for segment beginning with I in row n colunm 1 and TUNIT or DREGION etc in column3 \n",
    "    e.g. segment=[\"I\",\"TUNIT\",None,\"1\"] for power generator data\n",
    "    e.g. segment=[\"I\",\"DREGION\",None,\"2\"] for state totals\n",
    "    \"\"\"\n",
    "    #reader = csv.DictReader(file_obj, delimiter=',', dialect=csv.unix_dialect)\n",
    "    reader = csv.reader(file_obj, delimiter=',', dialect=csv.unix_dialect)\n",
    "    lines=[]\n",
    "    #save contents? only when found valid header\n",
    "    valid=False\n",
    "    for row, line in enumerate(reader):\n",
    "        #print(row, line[0:9])          #Uncomment to debug.\n",
    "        #Find \"I\" NemWeb header rows for 'T-Units'\n",
    "        if line[0]==segment[0] and line[1]==segment[1] and line[3]==segment[3]:\n",
    "            print(\"Found Start at:\",row, line[0:9])\n",
    "            valid=True\n",
    "            start=row\n",
    "            headers = line\n",
    "        if line[0]==segment[0] and valid and row>start: # and line[1]==\"TUNIT\" and line[3]==\"2\":\n",
    "            print(\"Found next header at\",row,\"exiting with\",row-start,\"lines\")\n",
    "            break\n",
    "            \n",
    "        if valid and not line[0]=='I':\n",
    "            # save non header row\n",
    "            lines.append(line)\n",
    "    return lines, headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create list of file paths\n",
    "csv_paths = [unzip_dir+f for f in f_unzip[0:]]  # 0:3 for 3 files\n",
    "print(csv_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create empty lists\n",
    "newlines = lines = header = all_lines = []\n",
    "\n",
    "for csv_path in csv_paths:\n",
    "    print(\"Processing\",csv_path)\n",
    "    with open(csv_path, \"r\", newline='\\n') as f_obj:\n",
    "        # send file object to reader and extract valid rows of data\n",
    "        newlines, header = csv_reader(f_obj,segment=[\"I\",\"DREGION\",None,\"2\"])\n",
    "        lines.extend(newlines)\n",
    "        #note lines.append creates a list of lists, rather than just one long list.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now we could create new csv, store data in hadoop, or in a dataframe\n",
    "print(header)\n",
    "print(lines[0])\n",
    "\n",
    "df = pd.DataFrame(data=lines, columns=header)\n",
    "\n",
    "# Define our index as date and time column\n",
    "df.set_index(\"SETTLEMENTDATE\", inplace=True)\n",
    "df.index = pd.to_datetime(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chart if you must?\n",
    "#df.pivot(columns='DUID', values='TOTALCLEARED').astype(float).plot(title=\"Energy\",legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import display\n",
    "display(df.iloc[0:11,[0,5,6,7,11,12]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save to Hadoop File for fast loading later\n",
    "df.to_hdf('AllRows.hdf5',key='NRG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nsw = df.loc[df.loc[:,\"REGIONID\"]==\"NSW1\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(nsw[\"TOTALDEMAND\"].astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "daily = nsw[\"TOTALDEMAND\"].astype(float).fillna(0).resample(rule=\"D\").sum().dropna()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "daily.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Alternatively, for simple csv file formats with only 1 header row we can add all directory contents to a single dataframe\n",
    "# Combine data into dataframe\n",
    "def df_from_many(dir_path, file_type=\".CSV\", date_col='DATE'):\n",
    "    #Setup target pandas dataframe\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    #Loop over cwd/data zip files\n",
    "    print(os.getcwd())\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        #print(root,dirs,files)\n",
    "        for file in files:\n",
    "            if file.endswith(file_type):\n",
    "                print(file)\n",
    "                tempdf = pd.read_csv(dir_path+file, skiprows=1)\n",
    "                tempdf.index = pd.to_datetime(tempdf.loc[date_col,:])\n",
    "                df = pd.concat([df, tempdf], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
